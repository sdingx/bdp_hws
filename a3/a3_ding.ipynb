{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9ed1d0-bb8a-444a-86fa-c2c890fa8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa4e487b-c562-4732-b202-c2a375b9cf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.minExecutors', '1'),\n",
       " ('spark.dataproc.sql.joinConditionReorder.enabled', 'true'),\n",
       " ('spark.history.fs.logDirectory',\n",
       "  'gs://dataproc-temp-us-central1-360578915708-zif5fas7/69c1015c-6b85-43d5-bbaf-c3c1588d3cc3/spark-job-history'),\n",
       " ('spark.executor.memory', '5g'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.concurrent.write.enabled',\n",
       "  'false'),\n",
       " ('spark.yarn.historyServer.address', 'cluster-977d-m:18080'),\n",
       " ('spark.dataproc.sql.local.rank.pushdown.enabled', 'true'),\n",
       " ('spark.yarn.unmanagedAM.enabled', 'true'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'cluster-977d-m.us-central1-c.c.bdp-summer.internal.'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.app.id', 'application_1689896722086_0004'),\n",
       " ('spark.metrics.namespace',\n",
       "  'app_name:${spark.app.name}.app_id:${spark.app.id}'),\n",
       " ('spark.dataproc.sql.optimizer.leftsemijoin.conversion.enabled', 'true'),\n",
       " ('spark.hadoop.hive.execution.engine', 'mr'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10000'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip:/usr/lib/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.5-src.zip'),\n",
       " ('spark.app.name', 'Spark Updated Conf'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.sql.parquet.enableNestedColumnVectorizedReader', 'true'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.driver.port', '42781'),\n",
       " ('spark.sql.cbo.enabled', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.dataproc.sql.parquet.enableFooterCache', 'true'),\n",
       " ('spark.driver.maxResultSize', '2048m'),\n",
       " ('spark.sql.warehouse.dir', 'file:/spark-warehouse'),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '21m'),\n",
       " ('spark.yarn.am.memory', '640m'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1689896722086_0002'),\n",
       " ('spark.checkpoint.compress', 'true'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.dataproc.metrics.listener.metrics.collector.hostname',\n",
       "  'cluster-977d-m'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.submitTime', '1689898739390'),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://cluster-977d-m.us-central1-c.c.bdp-summer.internal:42253'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.extraListeners',\n",
       "  'com.google.cloud.spark.performance.DataprocMetricsListener'),\n",
       " ('spark.sql.cbo.joinReorder.enabled', 'true'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.app.startTime', '1689898952336'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://cluster-977d-m.us-central1-c.c.bdp-summer.internal.:8088/proxy/application_1689896722086_0004'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.sql.adaptive.enabled', 'true'),\n",
       " ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n",
       " ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '0'),\n",
       " ('spark.rpc.message.maxSize', '512'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.eventLog.dir',\n",
       "  'gs://dataproc-temp-us-central1-360578915708-zif5fas7/69c1015c-6b85-43d5-bbaf-c3c1588d3cc3/spark-job-history'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.driver.host', 'cluster-977d-m.us-central1-c.c.bdp-summer.internal'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create Spark session\n",
    "spark = SparkSession.builder.appName('HW3').getOrCreate()\n",
    "\n",
    "#change configuration settings on Spark \n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '5g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','8g')])\n",
    "\n",
    "#print spark configuration settings\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9999316b-9d6e-4178-bc47-da924ac5d96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"quote\", \"\\\"\")  \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "    .csv(\"gs://bdp-hw-su/notebooks/data/Crimes_-_2001_to_Present.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "671e6af8-c155-4b23-b39f-f9880a60fafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ID=11646166, Case Number='JC213529', Date='09/01/2018 12:01:00 AM', Block='082XX S INGLESIDE AVE', IUCR='0810', Primary Type='THEFT', Description='OVER $500', Location Description='RESIDENCE', Arrest=False, Domestic=True, Beat=631, District=6, Ward=8, Community Area=44, FBI Code='06', X Coordinate=None, Y Coordinate=None, Year=2018, Updated On='04/06/2019 04:04:43 PM', Latitude=None, Longitude=None, Location=None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7359a4f-e503-41c2-ae20-e8d5d29746aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 00:32:08 ERROR YarnClientSchedulerBackend: The YARN application has already ended! It might have been killed or the Application Master may have failed to start. Check the YARN application logs for more details.\n",
      "23/07/21 00:32:08 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.spark.SparkException: Unmanaged application application_1689896722086_0003 failed due to ApplicationMaster for attempt appattempt_1689896722086_0003_000001 timed out. Failing the application.\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:98) ~[spark-yarn_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:65) ~[spark-yarn_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:222) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:585) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n",
      "\tat jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?]\n",
      "\tat jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:?]\n",
      "\tat jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:490) ~[?:?]\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat java.lang.Thread.run(Thread.java:829) ~[?:?]\n",
      "23/07/21 00:32:08 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to send shutdown message before the AM has registered!\n",
      "23/07/21 00:32:08 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "23/07/21 00:32:08 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa3925-b4ab-4cd5-9da3-fdee76b8912d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
